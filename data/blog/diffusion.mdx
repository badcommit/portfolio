---
title: "From White Noise to Pure Innovation"
date: 2023-07-07
lastmod: "2023-07-07"
tags: ["ai", "cs"]
draft: false
summary: "a brief summary through the development of generative models"
layout: PostSimple
bibliography: references-data.bib
canonicalUrl: https://tailwind-nextjs-starter-blog.vercel.app/blog/new-features-in-v1/
animation: "RollingDice"
---

## Overview

Recently, diffusion model obtains a lot of attention in the deep learning community. Therefore, it is a time to learn what it is to absorb some new ideas

<TOCInline toc={props.toc} exclude="Overview" toHeading={2} />

## How to generate innovative image

One of the most common topics is how to enable deep neural networks to generate pictures.

However, several obstacles lie ahead. Firstly, we aim to generate high-resolution pictures while keeping the input relatively simple. This involves mapping low-dimensional data like gaussian noise to high-dimensional data, which poses a significant challenge.

The major problem here is that we lack knowledge of what high-dimensional data looks like, and unfortunately, it is discrete. As a result, points in the low-dimensional space are likely to map to missing regions in the high-dimensional space, leading to incomplete image generation.

To address this challenge, Variational Autoencoders (VAEs) were invented. We will discuss VAEs in more detail later.

Another crucial issue is the desire for the generated images to appear realistic and closely resemble what we observe. However, the definition of "real" in this context is ambiguous. If we knew a clear mathematical definition, especially one that is differentiable, it would increase the likelihood of following a reverse process to generate the picture. This presents a sort of "egg and chicken" problem since we need the real metric to back-propagate the loss during the generation process, while the generation process itself implies the real metric.

To tackle the ambiguity of "real" images, Generative Adversarial Networks (GANs) utilize a game-playing approach to solve the problem. GANs consist of two neural networks, a generator and a discriminator, which iteratively enhance the image generation process, resulting in images that progressively resemble reality.

## VAE

In the pursuit of mapping from low-dimension to high-dimension in deep learning, the decoder-encoder model has been a common and foundational approach. This involves using a chain of neural networks to compress the input into a low-dimensional space and then expand it back to high-dimensional space.

However, a key challenge arises when we want to generate new images from novel points in the low-dimensional space. We cannot be certain whether these points will map to valid high-dimensional points. One naive approach to address this is to use linear interpolation between two low-dimensional points to make the space smoother.

However, determining the appropriate output and loss for this linear interpolation presents a dilemma. Using the nearest sample point in the lower dimension as a basis for constructing the loss leads to a very flat low-dimensional space, imposing overly strong constraints.

Moreover, calculating the nearest point in the lower dimension requires dependency on the entire training result of the encoder, making convergence even more difficult.

To overcome these challenges, an alternative approach involves pre-assuming that each sample point in the low-dimensional space follows a certain distribution. The intuition behind this idea is that neighboring points should resemble mixtures of nearby sample points.

Variational Autoencoders (VAEs) address this problem by assuming a normal distribution for the low-dimensional space, as it simplifies the approach.

This transforms the similarity problem into a probability problem, where points with neighboring low-dimensional values (within one or two standard deviations) have a high chance of mapping back to the same high-dimensional point.

During the training process, VAEs sample nearby points of the original sample point, and then use the decoder to map these points back to the high-dimensional space. The newly generated points can be considered as adding extra noise to the low-dimensional space.

As a result, the decoder is compelled to learn a more generalized mapping to the high-dimensional space, enhancing the VAE's ability to generate meaningful and diverse images during the generation process.

$z_i \sim N(0,I)$

$q_\phi (z|x) = N(\mu_\phi(x), \sigma_\phi(x))$

$p_\theta (x|z) $

zi follows a normal distribution in the low dimension space. When we want to generate a new image, we just sample it.

q is a neural network we want it to learn how to encode the higher dimension into the lower dimension, hereby a normal distribution. Theta represents the parameter of the neural network.

The output of q phy as discussed before should be a probability, as normal distribution is easy to tackle, we let it output a mean and variance.

p is the decoder trying to map the latent (the low dimension space) back to high dimension space. It can be seen as a distribution, but as we use constructing loss, VAE just ignore the variance when outputing the final generating image. So it can be treated as a deterministic function.

The loss function is the sum of reconstruction loss and KL divergence.

$ L(\theta, \phi) = L_{rec} + L_{reg} = -E_{x \sim {data}}[E_{z \sim q_\phi(z' | x)} p_\theta( x |z') - x] + D_{KL}(E_{x \sim {data}} q_\phi(z | x) || N(0, I)) $

$ L(\theta, \phi) = L_{rec} + L_{reg} \le L_{rec} + E_{x \sim {data}} D_{KL}( q_\phi(z | x) || N(0, I))$

KL divergence is very nature way to measure the difference between two distribution. It is also easy to calculate.

## GAN

![GAN](/static/images/stable_diffusion/gan.png)

A common approach in the DL is that if we do not know something we create a nn to proxy it. Indeed, in the context of generative models, such as Generative Adversarial Networks (GANs), we seek a way to determine whether the generated image belongs to the same distribution as the real images. GANs are designed precisely to address this requirement.

In a GAN, two key components work in tandem: the discriminator and the generator.

1. Discriminator:
The discriminator is a neural network trained to distinguish between real and generated images. It acts as a classifier with the target of correctly identifying whether an image is real or fake. During training, it learns to differentiate the distribution of real images from the distribution of generated images.

2. Generator:
The generator, on the other hand, creates new images and aims to produce realistic images that can deceive the discriminator. It takes random noise or a latent vector as input and generates images. The generator is trained by trying to generate images that the discriminator classifies as real.

The Game:
The generator and discriminator form a game-like interaction. As they both share a common neural network architecture (part of which is used for back-propagation), they iteratively improve their abilities. The discriminator becomes more skilled at distinguishing real images from generated ones, while the generator gets better at producing images that resemble real data.

This adversarial process results in the generator becoming more proficient at generating images that align with the real data distribution, effectively learning the underlying data distribution from the real samples. The discriminator, in turn, improves its ability to distinguish real from generated images, making the entire model converge to a stable state.

The end goal is to have a generator that can produce images that are indistinguishable from real images to the discriminator, effectively capturing the true data distribution. GANs have shown remarkable success in generating high-quality images and have become a popular approach for various generative tasks in deep learning.

$\min_G \max_D V(G, D) = E_{x \sim data}[\log D(x)] + E_{z \sim N(0, I)}[\log(1-D(G(z)))]$

D is the discriminator network, G is the generator network. D should output the probability of the image is real. G should output the image.

It is not hard to proof that when it comes to the equilibrium, $G^* = arg min_G max_D V(G, D)$

$D_{KL}(P_{data}(x) || P_z(G^*(z)))$ should be minimized.

## Diffusion Model

![GAN](/static/images/stable_diffusion/diffusion.jpg)

The diffusion model closely resembles VAE but with a distinctive approach of adding noise directly to the image instead of the latent space.

This alteration in the training process relaxes the constraint on the latent space while necessitating additional efforts during training.

The key concept behind the diffusion model is to enable the model to discern and reverse the noise-added images back to their original forms. This is accomplished by iteratively applying the noise diffusion process and training the model to reconstruct the original image from the diffused, noisy version.

By mastering the process of distinguishing and reverting the effects of noise, the diffusion model can generate entirely new images from a starting point of total noise. This capability arises as the model effectively disentangles the noise from the underlying data representation through the chaining of noise addition steps.

Although the training process may be more involved and computationally intensive than traditional VAEs, the diffusion model has demonstrated remarkable success in generating high-quality and diverse images. This exciting alternative approach holds great potential in the field of generative models, complementing other methods such as VAEs and GANs.

### The forward process

$q(x_t | q_{x_{t-1}}) = N(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_tI)$

$q(x_{1: T} | x_0) = \prod^T q(x_t|x_{t-1})$

q is the probability of generated image given the previous image. To make it a simple, it is a stotastic process complying to a normal distribution, which just adding extra noise.

All the steps is independent to each other, so it can be treated as markov process.

$\beta$ is the variance of the normal distribution. It is a function of the time step in order to control the process of adding noise, because we do not want to add too much noise at the beginning.

Since it is normal distribution, we do not actually need to calculate the x_t by following the product rules, we can save time by calculating the closed form of it.

### The reverse process

The model needs to train the reverse process. $ p_\theta(x_{t-1} | x_{t}) = N(x_{t-1} | \mu_\theta(x_t, t), \beta_t)$ Since we add a little noise to the image, the reverse process could also be treated as normal distribution

Instead of training a model to output a denoise image $x_{t-1}$, we train it to acknowledge the noise and output the noise. This might be because the loss function will be simpler to write

However, the $p_\theta$ is hard to calculate, and we need some other helps to get the training loss directly by comparing the $p$ with model generated value

It is easier to calculate the $q(x_{t-1} | x_t, x_0)$ because it is equal with $q(x_{t-1} | x_t, x_0) = q(x_{t} | x_{t-1}, x_0) * q({x_{t-1}}| x_0) /  q(x_t | x_0)$

we will get a equation of $\mu_\theta(x_t, t) = Ax_t + Bx_0$ since we can map $x_0$ to $x_t$ using the forward process. The $\mu_\theta(x_t, t)$ could only depend on the $x_t$ and we compare this value with the output of neural network, we can train it to predict the noise

## Summary

These three models try to generate the image by different method.

VAE does it by trying to make latent space regularized, by adding a little bit of noise to it. Wishing that a regularized space can be easily mapped to the real distribution.

GAN did it by creating another network to push the generated high dimension space to the real distribution, hope it makes some reasonable overlapping.

Diffusion did it by creating a series of stop in the mapping process, hope that any point in the lower dimension following the process can jump to the real distribution.

Generally speaking, the direct manipulation of the latent space is harder to generate the real image, because we make a lot of assumptions of the unknown distribution.

More real samples could lead to a great result. That's why the diffusion model is more stable.
